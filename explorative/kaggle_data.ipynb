{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e12c216-bb71-4540-84ea-78b85b0fba9b",
   "metadata": {},
   "source": [
    "# Exploration of Kaggle Data\n",
    "This notebook can be used to create timeseries data from a kaggle data set of reddit post. It supports several options on how to construct the time series that can be found in the 'settings' section. The datasets this notebook has been tested on can be found under the following links:\n",
    "- [Kaggle Redit Dataset: dataisbeautiful](https://www.kaggle.com/datasets/unanimad/dataisbeautiful)\n",
    "- [Kaggle Wall Street Bets Dataset](https://www.kaggle.com/datasets/gpreda/reddit-wallstreetsbets-posts)\n",
    "#### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bd2f67fd-2998-4d47-97ed-228b80f83f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Dataset for Exploration (options: 'wsb_df', 'all_df')\n",
    "dataset = \"wsb_df\"\n",
    "# Select path to the data set\n",
    "path_to_data = \"../../Data/reddit_wsb.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e39e2d15-cae1-4f48-b7e3-2a27555b4cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the bin method (by 'seconds' or one 'day')\n",
    "bin_method = \"day\"\n",
    "\n",
    "# Select the bin size in seconds (e.g. 604800 for a week)\n",
    "bin_size = 604800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "335e7c89-c180-4df4-80ea-629783638b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select text processing method ('tfidf' or 'target_words')\n",
    "proc_method = 'target_words'\n",
    "\n",
    "# If word cound was selected, select a list of words to look for\n",
    "target_words = ['GME', 'GameStop']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfb9bf4-5831-4b0f-b9b5-0c808a692f93",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9632ffd5-8c2f-42d2-bf7f-6655ce40bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8bfa57a8-1b0a-4f57-80c8-c8b26a84f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wsb_df = pd.read_csv(\"../../Data/reddit_wsb.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f5cfb022-d403-41df-b467-44e5b240cce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which data set to use\n",
    "if dataset==\"wsb_df\":\n",
    "    # DataFrame with wsb posts\n",
    "    df= pd.read_csv(path_to_data)\n",
    "elif dataset==\"all_df\":\n",
    "    # DataFrame with reddit posts\n",
    "    df=pd.read_csv(path_to_data, low_memory=False)\n",
    "    df=df.rename({'created_utc': 'created'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b004aa-22c6-4fb9-9bfd-8d501eee5cde",
   "metadata": {},
   "source": [
    "#### Small Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54510c31-fc84-4d76-869d-79344bb95f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.fromtimestamp(df.created.min())\n",
    "end_date = datetime.fromtimestamp(df.created.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41c12a2b-cf06-4208-85f9-fe891490bb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is from 2020-09-29 02:46:56 to 2021-08-16 08:26:20 and has 53187 datapoints.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset is from {start_date} to {end_date} and has {df.shape[0]} datapoints.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac7bc8-aec4-41b5-a52a-057de572ef64",
   "metadata": {},
   "source": [
    "#### Data To Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9cfbb3b2-2ce7-45b1-87b3-4a61eabc69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all irelevant columns\n",
    "df = df[['title','score','created']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7833882f-57b9-46b4-9fba-a1ca82328055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning data into buckets of selected bin size\n",
    "dates = sorted(list(set(df['created'])))\n",
    "mini, maxi = min(dates),max(dates)\n",
    "# Puts datapoints into bins (depends on bin_method)\n",
    "if bin_method == 'seconds':\n",
    "    df['bins'] = df['created'].map(lambda timestamp:int((timestamp-mini)//bin_size))\n",
    "elif bin_method == 'day':\n",
    "    df['bins'] = df['created'].map(lambda timestamp: datetime.fromtimestamp(timestamp).date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b4c13440-cf3d-4a1e-b85f-9214288951ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_dict = {'title':'sum'}\n",
    "df = df.groupby(df['bins']).aggregate(aggregation_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec6c50-190b-436c-a6cb-6bce4b4bd0bf",
   "metadata": {},
   "source": [
    "#### Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b3dd182e-e899-4088-a401-6e9867e89ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if proc_method=='tfidf':\n",
    "    # Creates a tfidf vectorizer \n",
    "    tfidf = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        lowercase=True,\n",
    "        stop_words='english',\n",
    "        max_features=1000\n",
    "    )\n",
    "    # Creates tfidf matrix\n",
    "    features=tfidf.fit_transform(df['title']).toarray()   \n",
    "    index = df.index\n",
    "    # Creates a new datafram from the tfidf matrix\n",
    "    df = pd.DataFrame(\n",
    "        data=features,\n",
    "        columns=range(len(features[0])),\n",
    "        index=index,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "56320063-f6ae-4f87-8269-312169a86a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if proc_method=='target_words':\n",
    "    for word in target_words:\n",
    "        df[word] = df['title'].map(lambda title: title.count(word))\n",
    "    df['aggregat'] =  df.loc[:,[word for word in target_words]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e4788ae1-1a08-4097-a06d-727f09065bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[target_words+['aggregat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eb5acd5e-fed8-46fd-b6ce-bb9ab65ce93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GME</th>\n",
       "      <th>GameStop</th>\n",
       "      <th>aggregat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bins</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-09-29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-28</th>\n",
       "      <td>240</td>\n",
       "      <td>17</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-29</th>\n",
       "      <td>2943</td>\n",
       "      <td>124</td>\n",
       "      <td>3067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             GME  GameStop  aggregat\n",
       "bins                                \n",
       "2020-09-29     0         0         0\n",
       "2021-01-28   240        17       257\n",
       "2021-01-29  2943       124      3067"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f5f8ec-4ed6-4e1a-8e57-e572a31bd139",
   "metadata": {},
   "source": [
    "#### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "831c365a-73ef-4b74-a2e0-0bea2d824b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('wsb_time_series.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
